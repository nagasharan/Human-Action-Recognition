{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d1fedb-7336-4c81-b990-0524cf2f3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Activation, Dropout, Flatten, Dense, Input\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as plotty\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585d83e-1462-4d07-87bd-d4bf798ee00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./Human Action Recognition/Training_set.csv\")\n",
    "test_data = pd.read_csv(\"./Human Action Recognition/Testing_set.csv\")\n",
    "#print(train_data)\n",
    "print(train_data.label.value_counts())\n",
    "#actions = train_data.label.value_counts()\n",
    "#plt.pie(actions.values,actions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8268b7bc-501b-4ce7-9f69-7d4cebc6a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_graph():\n",
    "    actions_train = train_data.label.value_counts()\n",
    "    fig = plotty.pie(train_data, values=actions_train.values, names=actions_train.index, title='Human Activity Classifications')\n",
    "    fig.show()\n",
    "show_data_graph()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ea18b6-4326-406c-a5c7-b479b5f047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = train_data['filename']\n",
    "action = train_data['label']\n",
    "def train_data_random_images():\n",
    "    for i in range(6):\n",
    "        num = random.randint(1,12600)\n",
    "        img_name = \"Image_{}.jpg\".format(num)\n",
    "        train_fldr = \"./Human Action Recognition/train/\"\n",
    "        test_image = img.imread(train_fldr+img_name)\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(test_image)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"{}\".format(train_data.loc[train_data['filename'] == \"{}\".format(img_name), 'label'].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82947864-1b7f-4d07-98d2-231f2abb2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_random_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341ec5c-4ccd-44a0-b2cc-d7768e313e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "length = len(train_data)\n",
    "for i in (range(len(train_data))):\n",
    "    t = './Human Action Recognition/train/' + filename[i]    \n",
    "    temp_img = Image.open(t)\n",
    "    x_train.append(np.asarray(temp_img.resize((256,256))))\n",
    "x_train = x_train\n",
    "x_train = np.asarray(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d670bd4-e087-4d3a-86c6-62f479259122",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.asarray(train_data[\"label\"].factorize()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b0760396-3e95-4d34-bc94-fb5210ca1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(fn):\n",
    "    img = Image.open(fn)\n",
    "    return np.asarray(img.resize((256,256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3a079a03-3bc3-4481-b903-d446abc728f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_ = ['sitting','using_laptop','hugging','sleeping','drinking','clapping','dancing','cycling','calling','laughing','eating','fighting','listening_to_music','running','texting']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8c7b1-5823-453a-aacf-4dc819b76970",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=train_data.label.value_counts()\n",
    "num_classes=len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67364b-0f00-4d1d-b02b-38f13e1d6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Input(shape=(256,256,3)))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57c1758b-6a6e-4d94-a20f-cd281efec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1f4a269-8879-4301-aaa6-4a386eee0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 638ms/step - accuracy: 0.1122 - loss: 3.6877\n",
      "Epoch 2/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 711ms/step - accuracy: 0.2412 - loss: 2.3106\n",
      "Epoch 3/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 1s/step - accuracy: 0.3064 - loss: 2.0769\n",
      "Epoch 4/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 1s/step - accuracy: 0.3853 - loss: 1.9013\n",
      "Epoch 5/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 706ms/step - accuracy: 0.4313 - loss: 1.7344\n",
      "Epoch 6/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 748ms/step - accuracy: 0.5110 - loss: 1.5012\n",
      "Epoch 7/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 767ms/step - accuracy: 0.5945 - loss: 1.2541\n",
      "Epoch 8/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 756ms/step - accuracy: 0.6659 - loss: 1.0233\n",
      "Epoch 9/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 730ms/step - accuracy: 0.7480 - loss: 0.7768\n",
      "Epoch 10/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 662ms/step - accuracy: 0.8180 - loss: 0.5548\n",
      "Epoch 11/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 657ms/step - accuracy: 0.8529 - loss: 0.4618\n",
      "Epoch 12/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 664ms/step - accuracy: 0.8767 - loss: 0.3812\n",
      "Epoch 13/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 665ms/step - accuracy: 0.8888 - loss: 0.3559\n",
      "Epoch 14/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 659ms/step - accuracy: 0.9005 - loss: 0.3076\n",
      "Epoch 15/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 635ms/step - accuracy: 0.9176 - loss: 0.2624\n",
      "Epoch 16/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 622ms/step - accuracy: 0.9280 - loss: 0.2256\n",
      "Epoch 17/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 631ms/step - accuracy: 0.9356 - loss: 0.2052\n",
      "Epoch 18/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 632ms/step - accuracy: 0.9417 - loss: 0.1763\n",
      "Epoch 19/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 651ms/step - accuracy: 0.9363 - loss: 0.2063\n",
      "Epoch 20/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 640ms/step - accuracy: 0.9457 - loss: 0.1662\n",
      "Epoch 21/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 642ms/step - accuracy: 0.9367 - loss: 0.1981\n",
      "Epoch 22/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 634ms/step - accuracy: 0.9432 - loss: 0.1703\n",
      "Epoch 23/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 638ms/step - accuracy: 0.9477 - loss: 0.1783\n",
      "Epoch 24/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 657ms/step - accuracy: 0.9500 - loss: 0.1501\n",
      "Epoch 25/25\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 673ms/step - accuracy: 0.9511 - loss: 0.1643\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train, epochs=25,batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6ec49-ee3c-4014-842b-48c2f378dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "plt.plot(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd89553-79d0-4cb4-b220-7662dd30e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6591c-8c23-4f1b-a597-aa646511f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = []\n",
    "for i in range(100, 110):\n",
    "    imgg = \"Image_{}.jpg\".format(i)\n",
    "    train = \"./Human Action Recognition/train/\"\n",
    "    result = model.predict(np.asarray([read_img(train+imgg)]))\n",
    "    itemindex = np.where(result==np.max(result))\n",
    "    prediction = itemindex[1][0]\n",
    "    \n",
    "    r = random.randint(1, 60000)\n",
    "    \n",
    "    random_images.append((read_img(train+imgg), actions_[prediction]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1af55-990a-41a5-b47e-469fb20e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(list(map(lambda x: x[0], random_images)), list(map(lambda x: x[1], random_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "58e6e7e1-e194-4dd9-95ad-a76a68794950",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = x_train[250:]\n",
    "x_test_1 = x_train[:250]\n",
    "y_train_1 = y_train[250:]\n",
    "y_test_1 = y_train[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f06ed-af46-4633-8685-c038379979a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2=Sequential()\n",
    "\n",
    "model_2.add(Input(shape=(256,256,3)))\n",
    "model_2.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model_2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model_2.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model_2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model_2.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model_2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model_2.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model_2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model_2.add(Conv2D(256,(3,3),activation='relu'))\n",
    "model_2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(128,activation='relu'))\n",
    "model_2.add(Dense(256,activation='relu'))\n",
    "model_2.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2b930b1f-6cfa-43f8-84ec-eb9b0143895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c741f01c-19f0-468b-8830-b76c8b08050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 587ms/step - accuracy: 0.1587 - loss: 2.5372 - val_accuracy: 0.2280 - val_loss: 2.3308\n",
      "Epoch 2/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 615ms/step - accuracy: 0.2438 - loss: 2.2878 - val_accuracy: 0.2440 - val_loss: 2.2633\n",
      "Epoch 3/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 645ms/step - accuracy: 0.3115 - loss: 2.1388 - val_accuracy: 0.3000 - val_loss: 2.1479\n",
      "Epoch 4/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 657ms/step - accuracy: 0.3747 - loss: 1.9319 - val_accuracy: 0.3040 - val_loss: 2.1059\n",
      "Epoch 5/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 671ms/step - accuracy: 0.4344 - loss: 1.7435 - val_accuracy: 0.2680 - val_loss: 2.1503\n",
      "Epoch 6/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 675ms/step - accuracy: 0.4978 - loss: 1.5234 - val_accuracy: 0.3080 - val_loss: 2.2180\n",
      "Epoch 7/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 673ms/step - accuracy: 0.5911 - loss: 1.2325 - val_accuracy: 0.3200 - val_loss: 2.4345\n",
      "Epoch 8/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 683ms/step - accuracy: 0.6715 - loss: 1.0028 - val_accuracy: 0.2760 - val_loss: 2.5676\n",
      "Epoch 9/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 685ms/step - accuracy: 0.7295 - loss: 0.8283 - val_accuracy: 0.3080 - val_loss: 2.8211\n",
      "Epoch 10/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 681ms/step - accuracy: 0.8156 - loss: 0.5677 - val_accuracy: 0.2880 - val_loss: 3.5052\n",
      "Epoch 11/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 694ms/step - accuracy: 0.8526 - loss: 0.4476 - val_accuracy: 0.3040 - val_loss: 3.6634\n",
      "Epoch 12/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 686ms/step - accuracy: 0.8794 - loss: 0.3843 - val_accuracy: 0.3520 - val_loss: 3.6453\n",
      "Epoch 13/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 681ms/step - accuracy: 0.8954 - loss: 0.3133 - val_accuracy: 0.3200 - val_loss: 3.9441\n",
      "Epoch 14/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 701ms/step - accuracy: 0.9118 - loss: 0.2731 - val_accuracy: 0.3040 - val_loss: 4.5765\n",
      "Epoch 15/15\n",
      "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 691ms/step - accuracy: 0.9253 - loss: 0.2357 - val_accuracy: 0.2880 - val_loss: 4.8586\n"
     ]
    }
   ],
   "source": [
    "history_2=model_2.fit(x_train_1,y_train_1, validation_data=(x_test_1, y_test_1),epochs=15,batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6011bd4-5bab-4712-afd1-17ae5c150368",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images2 = []\n",
    "for i in range(1, 10):\n",
    "    imgg = \"Image_{}.jpg\".format(i)\n",
    "    train = \"./Human Action Recognition/train/\"\n",
    "    result = model.predict(np.asarray([read_img(train+imgg)]))\n",
    "    itemindex = np.where(result==np.max(result))\n",
    "    prediction = itemindex[1][0]\n",
    "    \n",
    "    r = random.randint(1, 60000)\n",
    "    \n",
    "    random_images2.append((read_img(train+imgg), actions_[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576bbd35-eecd-4db4-8d94-e2fd450a93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(list(map(lambda x: x[0], random_images2)), list(map(lambda x: x[1], random_images2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
